I'll create a build plan for the `device-operations` service that handles memory, inference, and training operations through an API. Let me first check if there are any instruction files I should follow.

## Build Plan for `device-operations` Service

### 1. **Project Structure**
```
node-manager/
â”œâ”€â”€ device-monitor/          (existing)
â”œâ”€â”€ device-operations/       (new)
â”‚   â”œâ”€â”€ DeviceOperations.csproj
â”‚   â”œâ”€â”€ Program.cs
â”‚   â”œâ”€â”€ appsettings.json
â”‚   â”œâ”€â”€ appsettings.Development.json
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ Controllers/
â”‚   â”‚   â”‚   â”œâ”€â”€ MemoryController.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ InferenceController.cs
â”‚   â”‚   â”‚   â””â”€â”€ TrainingController.cs
â”‚   â”‚   â”œâ”€â”€ Services/
â”‚   â”‚   â”‚   â”œâ”€â”€ Core/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ IDeviceService.cs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DirectMLService.cs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DeviceManagerService.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ Memory/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ IMemoryOperationsService.cs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ MemoryOperationsService.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ Inference/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ IInferenceService.cs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ InferenceService.cs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ModelLoaderService.cs
â”‚   â”‚   â”‚   â””â”€â”€ Training/
â”‚   â”‚   â”‚       â”œâ”€â”€ ITrainingService.cs
â”‚   â”‚   â”‚       â””â”€â”€ TrainingService.cs
â”‚   â”‚   â”œâ”€â”€ Models/
â”‚   â”‚   â”‚   â”œâ”€â”€ Requests/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MemoryAllocateRequest.cs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ InferenceRequest.cs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ TrainingRequest.cs
â”‚   â”‚   â”‚   â”œâ”€â”€ Responses/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MemoryStatusResponse.cs
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ InferenceResponse.cs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ TrainingStatusResponse.cs
â”‚   â”‚   â”‚   â””â”€â”€ Common/
â”‚   â”‚   â”‚       â”œâ”€â”€ DeviceInfo.cs
â”‚   â”‚   â”‚       â”œâ”€â”€ MemoryAllocation.cs
â”‚   â”‚   â”‚       â””â”€â”€ ModelMetadata.cs
â”‚   â”‚   â”œâ”€â”€ Middleware/
â”‚   â”‚   â”‚   â”œâ”€â”€ ExceptionHandlingMiddleware.cs
â”‚   â”‚   â”‚   â””â”€â”€ DeviceAvailabilityMiddleware.cs
â”‚   â”‚   â””â”€â”€ Extensions/
â”‚   â”‚       â”œâ”€â”€ ServiceCollectionExtensions.cs
â”‚   â”‚       â””â”€â”€ DirectMLExtensions.cs
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ DeviceOperations.Tests/
â””â”€â”€ node-manager.sln (updated)
```

### 2. **Technology Stack**
- **Framework**: ASP.NET Core 8.0 (Web API)
- **GPU Compute**: 
  - Microsoft.ML.OnnxRuntime.DirectML (for inference)
  - Vortice.Windows.Direct3D12 (for low-level memory ops)
  - Vortice.Windows.DirectML (for custom operations)
- **API Documentation**: Swashbuckle (Swagger/OpenAPI)
- **Logging**: Serilog
- **Testing**: xUnit, Moq
- **Database**: PostgreSQL (shared with device-monitor for device tracking)

### 3. **Core Dependencies** (NuGet)
```xml
<PackageReference Include="Microsoft.ML.OnnxRuntime.DirectML" Version="1.16.3" />
<PackageReference Include="Vortice.Windows" Version="3.3.4" />
<PackageReference Include="Vortice.Direct3D12" Version="3.3.4" />
<PackageReference Include="Swashbuckle.AspNetCore" Version="6.5.0" />
<PackageReference Include="Serilog.AspNetCore" Version="8.0.0" />
<PackageReference Include="Npgsql" Version="8.0.3" />
```

### 4. **API Endpoints Design**

#### Memory Operations
- `GET /api/memory/status` - Get current memory status for all devices
- `GET /api/memory/device/{deviceId}` - Get memory status for specific device
- `POST /api/memory/allocate` - Allocate memory on device
- `DELETE /api/memory/deallocate/{allocationId}` - Deallocate memory
- `POST /api/memory/copy` - Copy data between allocations
- `POST /api/memory/clear/{allocationId}` - Clear memory allocation

#### Inference Operations
- `POST /api/inference/load-model` - Load ONNX model to device
- `GET /api/inference/models` - List loaded models
- `POST /api/inference/run` - Run inference on loaded model
- `DELETE /api/inference/unload-model/{modelId}` - Unload model
- `GET /api/inference/status/{sessionId}` - Get inference status

#### Training Operations
- `POST /api/training/initialize` - Initialize training session
- `POST /api/training/step` - Execute training step
- `GET /api/training/status/{sessionId}` - Get training status
- `POST /api/training/checkpoint` - Save checkpoint
- `DELETE /api/training/session/{sessionId}` - End training session

### 5. **Implementation Phases**

#### Phase 1: Project Setup & Core Infrastructure âœ… COMPLETED
1. âœ… Create project structure
2. âœ… Set up ASP.NET Core Web API with Swagger
3. âœ… Implement DirectML initialization and device enumeration (REAL HARDWARE)
4. âœ… Create base service interfaces and dependency injection
5. âœ… Set up logging and exception handling middleware
6. âœ… Basic health check endpoint

#### Phase 2: Memory Operations âœ… COMPLETED
1. âœ… Implement Direct3D12 device management
2. âœ… Create memory allocation service
3. âœ… Implement memory status tracking
4. âœ… Add memory copy/clear operations
5. âœ… Create memory controller endpoints
6. âœ… Unit tests for memory operations

#### Phase 3: Inference Operations âœ… COMPLETED
1. âœ… Implement ONNX Runtime integration
2. âœ… Create model loader service
3. âœ… Implement inference session management
4. âœ… Add support for different model types (SDXL, VAE, etc.)
5. âœ… Create inference controller endpoints
6. âœ… Performance benchmarking

#### Phase 4: Training Operations âœ… COMPLETED
1. âœ… Implement training infrastructure
2. âœ… Create gradient computation service
3. âœ… Add checkpoint management
4. âœ… Implement training controller endpoints
5. âœ… Integration with inference for validation

#### Phase 5: Integration & Testing ðŸš€ IN PROGRESS
1. ðŸ”„ Integration with device-monitor service
2. ðŸ”„ End-to-end testing
3. ðŸ”„ Performance optimization
4. ðŸ”„ Documentation

### 6. **Key Design Decisions**

1. **Stateful Service**: Keep models and allocations in memory between requests for performance
2. **Device Abstraction**: Abstract DirectML/D3D12 behind interfaces for testability
3. **Async Operations**: All GPU operations should be async with status endpoints
4. **Resource Management**: Implement proper disposal and cleanup for GPU resources
5. **Multi-GPU Support**: Design APIs to handle multiple devices from the start

### 7. **Configuration Structure**
```json
{
  "DeviceOperations": {
    "MaxMemoryAllocationGB": 24,
    "DefaultDevice": "gpu_0",
    "ModelCachePath": "./models",
    "EnableGPUScheduling": true,
    "Inference": {
      "MaxConcurrentSessions": 5,
      "SessionTimeoutMinutes": 30
    },
    "Training": {
      "CheckpointPath": "./checkpoints",
      "MaxBatchSize": 32
    }
  }
}
```